# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.driver.memory              48g
spark.executor.memory            24g

##to control Netty dispatcher threads for workers/executors.
spark.rpc.netty.dispatcher.numThreads  10

##to control HTTP Server and Jetty Web Server dispatcher threads
spark.jetty.httpserver.numThreads 10

# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
# spark.cores.max         19


spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.kryo.referenceTracking     false
##spark.kryo.registrator           org.apache.spark.graphx.GraphKryoRegistrator
spark.eventLog.enabled           true
spark.eventLog.dir               /var/tmp/WORK_DIRS2/store-m0/eventLog
##NOTE: I point spark.local.dir to the store allocated to master node.
##each worker node/executor node will pick up its own from its spark-env.sh
##SPARK_LOCAL_DIRS
#spark.local.dir                         /var/tmp/WORK_DIRS2/store-m0/local
spark.local.dir                         /data
spark.log.Conf                          false
spark.eventLog.compress                 false
spark.ui.retainedStages                 10000
#spark.default.parallelism	 12
spark.executor.extraJavaOptions=-XX:-UseBiasedLocking
spark.driver.extraJavaOptions=-XX:-UseBiasedLocking


##to get around the kryo serializer buffer limit problem

spark.kryoserializer.buffer.max.mb 1024
#spark.executor.extraClassPath   /usr/local/hadoop/extras/jblas-master/target/jblas-1.2.4-SNAPSHOT.jar
# spark.executor.extraLibraryPath /usr/lib64

##NOTE: for SHM shuffle engine, for spark hpc package, we will need to specify full shm shuffle manager class path.
spark.shuffle.manager org.apache.spark.shuffle.shm.ShmShuffleManager
#spark.shuffle.manager sort
spark.shuffle.shm.serializer.buffer.max.mb 1024
##for new RMB version ALPS
spark.executor.shm.globalheap.name /lfs/spark_global0
#spark.executor.shm.globalheap.name /dev/shm/nvm/global0
##for off-heap store
spark.offheapstore.globalheap.name  /lfs/spark_global0

##WARNING: SPARK_WORKER_CORES, for thread-based concurrent hash map entry assignment, is done at spark-env.sh
##NOTE: I just found that it does not work at all and it confuses with the same parameter at
##spark-env.sh. DONOT use this!! 
###SPARK_WORKER_CORES 9

# spark.sql.dialect sql
# hiveql parser is more robust and allows access to hive udfs.
##spark.sql.dialect hiveql

# this is needed when creating files/tables and switching between spark-scala-shell,
# spark-sql-shell, and jdbc such that they can all read/write the parquet files correctly
# to share among all shell flavors.
spark.sql.parquet.binaryAsString true
##spark.sql.hive.version=0.12.0

spark.executor.extraClassPath   /var/tmp/spark2.0hpcplatform/lib/firesteel-2.4.0.jar
spark.driver.extraClassPath   /var/tmp/spark2.0hpcplatform/lib/firesteel-2.4.0.jar

# Enable JMX for remotely attaching VisualVM. 
# We need to assign a specific port to JMX as Docker does not provide any way
# to open a port dynamically. Assigning a fixed port is okay when running a 
# single application at a time (single executor per worker) but it won't work 
# when running multiple applications concurrently (multiple executors will 
# conflict for the same fixed port)
##spark.executor.extraJavaOptions -XX:ParallelGCThreads=10 -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=40000  -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false
spark.executor.extraJavaOptions -XX:ParallelGCThreads=10

spark.driver.maxResultSize   0
