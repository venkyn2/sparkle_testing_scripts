Spark Executor Command: "/opt/jdk1.8.0_131/jre/bin/java" "-cp" "/var/tmp/spark2.0hpcplatform/lib/firesteel-2.4.0.jar:/var/tmp/spark2.0hpcplatform/multicore/worker0/conf/:/var/tmp/spark2.0hpcplatform/multicore/worker0/assembly/target/scala-2.11/jars/*:/opt/hadoop/etc/hadoop/" "-Xmx15360M" "-Dspark.driver.port=38757" "-Dspark.rpc.netty.dispatcher.numThreads=10" "-XX:-UseBiasedLocking" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@mdc-ch1-cust4.mdc.ext.hpe.com:38757" "--executor-id" "54" "--hostname" "10.1.1.4" "--cores" "4" "--app-id" "app-20190827081035-0017" "--worker-url" "spark://Worker@10.1.1.4:46217"
========================================

19/08/27 08:10:35 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 48597@mdc-ch1-cust4
19/08/27 08:10:35 INFO util.SignalUtils: Registered signal handler for TERM
19/08/27 08:10:35 INFO util.SignalUtils: Registered signal handler for HUP
19/08/27 08:10:35 INFO util.SignalUtils: Registered signal handler for INT
19/08/27 08:10:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/08/27 08:10:36 INFO spark.SecurityManager: Changing view acls to: root,nnnnnven
19/08/27 08:10:36 INFO spark.SecurityManager: Changing modify acls to: root,nnnnnven
19/08/27 08:10:36 INFO spark.SecurityManager: Changing view acls groups to: 
19/08/27 08:10:36 INFO spark.SecurityManager: Changing modify acls groups to: 
19/08/27 08:10:36 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root, nnnnnven); groups with view permissions: Set(); users  with modify permissions: Set(root, nnnnnven); groups with modify permissions: Set()
19/08/27 08:10:36 INFO client.TransportClientFactory: Successfully created connection to mdc-ch1-cust4.mdc.ext.hpe.com/10.1.1.4:38757 after 47 ms (0 ms spent in bootstraps)
19/08/27 08:10:36 WARN spark.SparkConf: The configuration key 'spark.kryoserializer.buffer.max.mb' has been deprecated as of Spark 1.4 and may be removed in the future. Please use the new key 'spark.kryoserializer.buffer.max' instead.
19/08/27 08:10:36 INFO spark.SecurityManager: Changing view acls to: root,nnnnnven
19/08/27 08:10:36 INFO spark.SecurityManager: Changing modify acls to: root,nnnnnven
19/08/27 08:10:36 INFO spark.SecurityManager: Changing view acls groups to: 
19/08/27 08:10:36 INFO spark.SecurityManager: Changing modify acls groups to: 
19/08/27 08:10:36 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root, nnnnnven); groups with view permissions: Set(); users  with modify permissions: Set(root, nnnnnven); groups with modify permissions: Set()
19/08/27 08:10:36 INFO client.TransportClientFactory: Successfully created connection to mdc-ch1-cust4.mdc.ext.hpe.com/10.1.1.4:38757 after 2 ms (0 ms spent in bootstraps)
19/08/27 08:10:36 INFO storage.DiskBlockManager: Created local directory at /data/spark-6e9264d8-6fb2-4eff-a979-6d4b27dc7a35/executor-605bf3b0-8b6b-4a4d-922e-2ea5d1595958/blockmgr-001ab5db-34ca-4c21-b082-c51a436564d3
19/08/27 08:10:36 INFO memory.MemoryStore: MemoryStore started with capacity 7.8 GB
19/08/27 08:10:36 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@mdc-ch1-cust4.mdc.ext.hpe.com:38757
19/08/27 08:10:37 INFO worker.WorkerWatcher: Connecting to worker spark://Worker@10.1.1.4:46217
19/08/27 08:10:37 INFO client.TransportClientFactory: Successfully created connection to /10.1.1.4:46217 after 1 ms (0 ms spent in bootstraps)
19/08/27 08:10:37 INFO worker.WorkerWatcher: Successfully connected to spark://Worker@10.1.1.4:46217
19/08/27 08:10:37 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
19/08/27 08:10:37 INFO executor.Executor: Starting executor ID 54 on host 10.1.1.4
19/08/27 08:10:37 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43437.
19/08/27 08:10:37 INFO netty.NettyBlockTransferService: Server created on 10.1.1.4:43437
19/08/27 08:10:37 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/08/27 08:10:37 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(54, 10.1.1.4, 43437, None)
19/08/27 08:10:37 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(54, 10.1.1.4, 43437, None)
19/08/27 08:10:37 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(54, 10.1.1.4, 43437, None)
19/08/27 08:10:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8
19/08/27 08:10:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9
19/08/27 08:10:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10
19/08/27 08:10:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11
19/08/27 08:10:37 INFO executor.Executor: Running task 8.0 in stage 0.0 (TID 8)
19/08/27 08:10:37 INFO executor.Executor: Running task 11.0 in stage 0.0 (TID 11)
19/08/27 08:10:37 INFO executor.Executor: Running task 10.0 in stage 0.0 (TID 10)
19/08/27 08:10:37 INFO executor.Executor: Running task 9.0 in stage 0.0 (TID 9)
19/08/27 08:10:37 INFO executor.Executor: Fetching spark://mdc-ch1-cust4.mdc.ext.hpe.com:38757/jars/spark-terasort-1.1-SNAPSHOT-jar-with-dependencies.jar with timestamp 1566918635331
19/08/27 08:10:37 INFO client.TransportClientFactory: Successfully created connection to mdc-ch1-cust4.mdc.ext.hpe.com/10.1.1.4:38757 after 1 ms (0 ms spent in bootstraps)
19/08/27 08:10:37 INFO util.Utils: Fetching spark://mdc-ch1-cust4.mdc.ext.hpe.com:38757/jars/spark-terasort-1.1-SNAPSHOT-jar-with-dependencies.jar to /data/spark-6e9264d8-6fb2-4eff-a979-6d4b27dc7a35/executor-605bf3b0-8b6b-4a4d-922e-2ea5d1595958/spark-f6ab97e6-d15d-4d2f-b9d7-ed8867a73b18/fetchFileTemp2859650591287294875.tmp
19/08/27 08:10:37 INFO util.Utils: Copying /data/spark-6e9264d8-6fb2-4eff-a979-6d4b27dc7a35/executor-605bf3b0-8b6b-4a4d-922e-2ea5d1595958/spark-f6ab97e6-d15d-4d2f-b9d7-ed8867a73b18/9965611231566918635331_cache to /var/tmp/spark2.0hpcplatform/multicore/worker0/work/app-20190827081035-0017/54/./spark-terasort-1.1-SNAPSHOT-jar-with-dependencies.jar
19/08/27 08:10:37 INFO executor.Executor: Adding file:/var/tmp/spark2.0hpcplatform/multicore/worker0/work/app-20190827081035-0017/54/./spark-terasort-1.1-SNAPSHOT-jar-with-dependencies.jar to class loader
19/08/27 08:10:37 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
19/08/27 08:10:37 INFO client.TransportClientFactory: Successfully created connection to mdc-ch1-cust4.mdc.ext.hpe.com/10.1.1.4:35579 after 1 ms (0 ms spent in bootstraps)
19/08/27 08:10:37 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.4 KB, free 7.8 GB)
19/08/27 08:10:37 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 67 ms
19/08/27 08:10:37 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 68.5 KB, free 7.8 GB)
19/08/27 08:10:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/08/27 08:10:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/08/27 08:10:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/08/27 08:10:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/08/27 08:10:38 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/08/27 08:10:38 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/08/27 08:10:38 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/08/27 08:10:38 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
19/08/27 08:29:22 WARN hdfs.DFSClient: DataStreamer Exception
java.io.IOException: Broken pipe
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:65)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.hdfs.DFSPacket.writeTo(DFSPacket.java:176)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:508)
19/08/27 08:29:22 WARN hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-1096908835-10.1.1.4-1566790228469:blk_1073749878_9054
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2278)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:244)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:734)
19/08/27 08:29:22 WARN hdfs.DFSClient: DataStreamer Exception
java.io.IOException: Broken pipe
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:65)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.hdfs.DFSPacket.writeTo(DFSPacket.java:176)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:508)
19/08/27 08:29:22 WARN hdfs.DFSClient: DataStreamer Exception
java.io.IOException: Broken pipe
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:65)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.hdfs.DFSPacket.writeTo(DFSPacket.java:176)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:508)
19/08/27 08:29:22 WARN hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-1096908835-10.1.1.4-1566790228469:blk_1073749849_9025
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2278)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:244)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:734)
19/08/27 08:29:22 WARN hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-1096908835-10.1.1.4-1566790228469:blk_1073749855_9031
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2278)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:244)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:734)
19/08/27 08:29:22 ERROR util.Utils: Aborting task
java.io.IOException: All datanodes DatanodeInfoWithStorage[10.1.4.4:50010,DS-5a8ee854-26dd-4c76-a2a6-837080893353,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1084)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:876)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:402)
19/08/27 08:29:22 ERROR util.Utils: Aborting task
java.io.IOException: All datanodes DatanodeInfoWithStorage[10.1.4.4:50010,DS-5a8ee854-26dd-4c76-a2a6-837080893353,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1084)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:876)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:402)
19/08/27 08:29:22 ERROR util.Utils: Aborting task
java.io.IOException: All datanodes DatanodeInfoWithStorage[10.1.4.4:50010,DS-5a8ee854-26dd-4c76-a2a6-837080893353,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1084)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:876)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:402)
19/08/27 08:29:22 ERROR io.SparkHadoopWriter: Task attempt_20190827081035_0001_r_000010_0 aborted.
19/08/27 08:29:22 ERROR io.SparkHadoopWriter: Task attempt_20190827081035_0001_r_000008_0 aborted.
19/08/27 08:29:22 ERROR io.SparkHadoopWriter: Task attempt_20190827081035_0001_r_000011_0 aborted.
19/08/27 08:29:22 ERROR executor.Executor: Exception in task 11.0 in stage 0.0 (TID 11)
org.apache.spark.SparkException: Task failed while writing rows
	at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:155)
	at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)
	at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: All datanodes DatanodeInfoWithStorage[10.1.4.4:50010,DS-5a8ee854-26dd-4c76-a2a6-837080893353,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1084)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:876)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:402)
19/08/27 08:29:22 ERROR executor.Executor: Exception in task 10.0 in stage 0.0 (TID 10)
org.apache.spark.SparkException: Task failed while writing rows
	at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:155)
	at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)
	at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: All datanodes DatanodeInfoWithStorage[10.1.4.4:50010,DS-5a8ee854-26dd-4c76-a2a6-837080893353,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1084)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:876)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:402)
19/08/27 08:29:22 ERROR executor.Executor: Exception in task 8.0 in stage 0.0 (TID 8)
org.apache.spark.SparkException: Task failed while writing rows
	at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:155)
	at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)
	at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: All datanodes DatanodeInfoWithStorage[10.1.4.4:50010,DS-5a8ee854-26dd-4c76-a2a6-837080893353,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1084)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:876)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:402)
19/08/27 08:31:07 INFO hdfs.DFSClient: Exception in createBlockOutputStream
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1508)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1284)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1237)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:449)
19/08/27 08:31:07 INFO hdfs.DFSClient: Abandoning BP-1096908835-10.1.1.4-1566790228469:blk_1073750095_9271
19/08/27 08:31:07 INFO hdfs.DFSClient: Excluding datanode DatanodeInfoWithStorage[10.1.4.4:50010,DS-5a8ee854-26dd-4c76-a2a6-837080893353,DISK]
19/08/27 08:56:24 INFO hdfs.DFSClient: Removing node DatanodeInfoWithStorage[10.1.4.4:50010,DS-5a8ee854-26dd-4c76-a2a6-837080893353,DISK] from the excluded nodes list
19/08/27 09:00:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_20190827081035_0001_r_000009_0' to hdfs://mdc-ch1-cust4:9000/user/nnnnnven5000G/HSsort-input/_temporary/0/task_20190827081035_0001_r_000009
19/08/27 09:00:58 INFO mapred.SparkHadoopMapRedUtil: attempt_20190827081035_0001_r_000009_0: Committed
19/08/27 09:00:58 INFO executor.Executor: Finished task 9.0 in stage 0.0 (TID 9). 912 bytes result sent to driver
19/08/27 09:20:14 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
19/08/27 09:20:14 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
tdown
